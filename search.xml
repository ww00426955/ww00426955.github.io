<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[pytorch之维度变化]]></title>
    <url>%2F2019%2F07%2F22%2Fpytorch%E4%B9%8B%E7%BB%B4%E5%BA%A6%E5%8F%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[reshape、view和transpose、permute的区别 reshape和view只是变换形状,而transpose和permute是变换维度，变换结果不一样！ reshape和view变换后的得到的tensor都是连续的，而transpose和permute是不连续的 若x是连续的tensor，则x的改变都会影响以上的变换结果，因为都不是x的副本 1234567891011121314151617181920212223242526272829303132333435import torchtorch.manual_seed(1)x = torch.randint(0, 2, (2, 3))y_reshape = x.reshape(3, 2)y_view = x.view(3, 2)print(y_reshape.is_contiguous()) # Trueprint(y_view.is_contiguous()) # Truey_transpose = x.transpose(0, 1)y_permute = x.permute(1, 0)print(y_transpose.is_contiguous()) # Falseprint(y_permute.is_contiguous()) # Falsex[1, 1] = 99print(x)# tensor([[ 1, 1, 0],# [ 0, 99, 1]])print(y_reshape)# tensor([[ 1, 1],# [ 0, 0],# [99, 1]])print(y_view)# tensor([[ 1, 1],# [ 0, 0],# [99, 1]])print(y_transpose)# tensor([[ 1, 0],# [ 1, 99],# [ 0, 1]])print(y_permute)# tensor([[ 1, 0],# [ 1, 99],# [ 0, 1]]) reshape和view的区别实验一： view使用前要求tensor是连续contiguous的， 若tensor是不连续的，则view之前必须调用contiguous()方法使之变成连续的tensor permute之后使用contiguous()： 返回的tensor是x的copy，x变化时，不会随着改变 返回的tensor是连续的，可以接着使用view变换 若y_permute = x.permute(1, 0).contiguous()改为y_permute = x.permute(1, 0)则结论和上述相反，x变化，y_permute跟着变化，view不可以接着使用 1234567891011121314151617181920import torchtorch.manual_seed(1)x = torch.randint(0, 2, (2, 3))y_permute = x.permute(1, 0).contiguous()print(y_permute.is_contiguous()) # Truex[1, 1] = 99print(x)# tensor([[ 1, 1, 0],# [ 0, 99, 1]])print(y_permute)# tensor([[1, 0],# [1, 1],# [0, 1]])y_view = y_permute.view(-1)print(y_view)# tensor([1, 0, 1, 1, 0, 1]) 实验二： tensor是不连续的时候，reshape依然会返回新的copy，view报错 tensor是连续的时候，view和reshape都不返回新的copy，跟随改变 view和reshape最终返回的一定是连续的tensor 123456789101112131415161718192021import torchtorch.manual_seed(1)x = torch.randint(0, 2, (2, 3))y_permute = x.permute(1, 0)print(y_permute.is_contiguous()) # Falsey_reshape = y_permute.reshape(1, 6)print(y_reshape.is_contiguous()) # Truex[1, 1] = 99print(x)# tensor([[ 1, 1, 0],# [ 0, 99, 1]])print(y_permute)# tensor([[ 1, 0],# [ 1, 99],# [ 0, 1]])print(y_reshape)# tensor([[1, 0, 1, 1, 0, 1]])]]></content>
  </entry>
  <entry>
    <title><![CDATA[torch.gather用法]]></title>
    <url>%2F2019%2F07%2F22%2Ftorch.gather%2F</url>
    <content type="text"><![CDATA[For a 1-D tensor计算公式： 1output[i] = input[index[i]] # dim=0 1234import torchinput = torch.tensor([1, 2, 3]) # shape = [3]index = torch.tensor([0, 1, 1, 1, 1, 1, 2]) # shape = [n]output = input.gather(dim=0, index=index) # tensor([1, 2, 2, 2, 2, 2, 3]) For a 2-D tensor计算公式： 12output[i][j] = input[index[i][j]][j] if dim=0output[i][j] = input[i][index[i][j]] if dim=1 1234567891011121314151617181920212223242526272829import torchinput = torch.tensor([[1, 2, 3], [4, 5, 6]]) # shape = [2,3]index_dim_0 = torch.tensor([[0, 1, 0]]) # shape = [1,3]output = torch.gather(input, dim=0, index=index_dim_0) # tensor([[1, 5, 3]])index_dim_0 = torch.tensor([[0, 1, 0], # shape = [2,3] [1, 0, 1]])# [[1, 5, 3],# [4, 2, 6]]output = torch.gather(input, dim=0, index=index_dim_0)index_dim_1 = torch.tensor([[0], # shape = [2,1] [2]])# tensor([[1],# [6]])output = torch.gather(input, dim=1, index=index_dim_1)index_dim_1 = torch.tensor([[0, 2], # shape = [2,2] [1, 0]])# tensor([[1, 3],# [5, 4]])output = torch.gather(input, dim=1, index=index_dim_1)index_dim_1 = torch.tensor([[2, 1, 0], # shape = [2,3] [1, 0, 2]])# tensor([[3, 2, 1],# [5, 4, 6]])output = torch.gather(input, dim=1, index=index_dim_1) For a 3-D tensor计算公式： 123out[i][j][k] = input[index[i][j][k]][j][k] # if dim == 0out[i][j][k] = input[i][index[i][j][k]][k] # if dim == 1out[i][j][k] = input[i][j][index[i][j][k]] # if dim == 2 总结 output.shape == index.shape input.shape和index.shape，在dim != d的维度上要保持一致，dim=d的维度任意]]></content>
  </entry>
  <entry>
    <title><![CDATA[分类Loss总结]]></title>
    <url>%2F2019%2F07%2F22%2F%E5%88%86%E7%B1%BBLoss%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[log_softmax1234567891011import torchimport torch.nn as nnimport torch.nn.functional as Fx = torch.FloatTensor([[1, 2, 3], [4, 5, 6]]) # batch=2,C=3,shape=[2,3]y_ours = torch.exp(x)y_ours = torch.log(y_ours / y_ours.sum(-1).reshape(2, 1))y_log_soft_max = F.log_softmax(x, dim=1)print(y_ours)print(y_log_soft_max) BCELoss、BCEWithLogitsLossBCELoss 公式： 用于二分类问题 BCEWithLogitsLoss： 等价于 sigmoid + BCELoss，使得数值计算更加稳定 123456789101112import torchimport torch.nn as nninput = torch.randn(3, 3)target = torch.empty(3, 3).random_(2)loss_bce = nn.BCELoss()output1 = loss_bce(torch.sigmoid(input), target)loss_bcel = nn.BCEWithLogitsLoss()output2 = loss_bcel(input, target)print(output1, output2) NLLLoss、 CrossEntropyLossNLLLoss： 三个重要参数：weight，reduction，ignore_index 通过设置weight参数，可以灵活调控不同类别的loss权重，解决不同类别的数量不均衡问题： 图像分类：不同的图像类别 语义分割：不同的像素类别，如：背景像素、前景像素；医学图像中肝脏像素和血管像素等 12345678910111213141516171819202122232425262728293031323334353637383940414243x = torch.FloatTensor([[1, 2, 3], [4, 5, 6]]) # batch=2,C=3,shape=[2,3]y_log_soft_max = F.log_softmax(x, dim=1)label = torch.LongTensor([0, 1])# reduction noneloss_nll = nn.NLLLoss(reduction='none')loss = loss_nll(y_log_soft_max, label)print(loss) # [2.4076, 1.4076]# reduction meanloss_nll = nn.NLLLoss(reduction='mean')loss = loss_nll(y_log_soft_max, label)print(loss) # 1.9076# reduction sumloss_nll = nn.NLLLoss(reduction='sum')loss = loss_nll(y_log_soft_max, label)print(loss) # 3.8152# ignore_indexloss_nll = nn.NLLLoss(reduction='none', ignore_index=0)loss = loss_nll(y_log_soft_max, label)print(loss) # [0.0000, 1.4076]loss_nll = nn.NLLLoss(reduction='none', ignore_index=1)loss = loss_nll(y_log_soft_max, label)print(loss) # [2.4076, 0.0000]# weightweight = torch.FloatTensor([2.0, 1.0, 3.0]) # class weightloss_nll = nn.NLLLoss(weight=weight, reduction='none')loss = loss_nll(y_log_soft_max, label)print(loss) # [2.4076*2.0, 1.4076*1.0]loss_nll = nn.NLLLoss(weight=weight, reduction='mean')loss = loss_nll(y_log_soft_max, label)print(loss) # (2.4076*2.0 + 1.4076*1.0) / (2.0 + 1.0) = 2.0743loss_nll = nn.NLLLoss(weight=weight, reduction='sum')loss = loss_nll(y_log_soft_max, label)print(loss) # 6.2228 CrossEntropyLoss： 等价于 log_softmax + NLLLoss 123456789x = torch.FloatTensor([[1, 2, 3], [4, 5, 6]]) # batch=2,C=3,shape=[2,3]label = torch.LongTensor([0, 1])loss_cross_entropy = nn.CrossEntropyLoss()print(loss_cross_entropy(x, label)) # 1.9076loss_nll = nn.NLLLoss()print(loss_nll(F.log_softmax(x, dim=1), label)) # 1.9076 Segmentation 2d/3d分类loss在分割问题上的应用： 12345678910111213141516171819202122232425import torchimport torch.nn as nnimport torch.nn.functional as F# 2D input is of size N x C x height x widthN, C = 4, 10x = torch.randn(N, 1, 64, 64)target = torch.empty(N, 64, 64, dtype=torch.long).random_(0, C)model = nn.Conv2d(1, C, (3, 3), padding=1)output = model(x) # [4, 10, 64, 64]loss_nll = nn.NLLLoss()loss = loss_nll(F.log_softmax(output, dim=1), target)# 3D input is of size N x C x height x width × ZN, C = 4, 10x = torch.randn(N, 1, 64, 64, 8)target = torch.empty(N, 64, 64, 8, dtype=torch.long).random_(0, C)model = nn.Conv3d(1, C, (3, 3, 3), padding=1)output = model(x) # [4, 10, 64, 64, 8]loss_nll = nn.NLLLoss()loss = loss_nll(F.log_softmax(output, dim=1), target)]]></content>
  </entry>
</search>
