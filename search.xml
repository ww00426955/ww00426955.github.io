<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>pytorch之维度变化</title>
      <link href="/2019/07/22/pytorch%E4%B9%8B%E7%BB%B4%E5%BA%A6%E5%8F%98%E5%8C%96/"/>
      <url>/2019/07/22/pytorch%E4%B9%8B%E7%BB%B4%E5%BA%A6%E5%8F%98%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<h2 id="reshape、view和transpose、permute的区别"><a href="#reshape、view和transpose、permute的区别" class="headerlink" title="reshape、view和transpose、permute的区别"></a>reshape、view和transpose、permute的区别</h2><ul><li>reshape和view只是变换形状,而transpose和permute是变换维度，变换结果不一样！</li><li>reshape和view变换后的得到的tensor都是连续的，而transpose和permute是不连续的</li><li>若x是连续的tensor，则x的改变都会影响以上的变换结果，因为都不是x的副本</li></ul><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line">x = torch.randint(<span class="number">0</span>, <span class="number">2</span>, (<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">y_reshape = x.reshape(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">y_view = x.view(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">print(y_reshape.is_contiguous())  <span class="comment"># True</span></span><br><span class="line">print(y_view.is_contiguous())  <span class="comment"># True</span></span><br><span class="line"></span><br><span class="line">y_transpose = x.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">y_permute = x.permute(<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">print(y_transpose.is_contiguous())  <span class="comment"># False</span></span><br><span class="line">print(y_permute.is_contiguous())  <span class="comment"># False</span></span><br><span class="line"></span><br><span class="line">x[<span class="number">1</span>, <span class="number">1</span>] = <span class="number">99</span></span><br><span class="line">print(x)</span><br><span class="line"><span class="comment"># tensor([[ 1,  1,  0],</span></span><br><span class="line"><span class="comment">#         [ 0, 99,  1]])</span></span><br><span class="line">print(y_reshape)</span><br><span class="line"><span class="comment"># tensor([[ 1,  1],</span></span><br><span class="line"><span class="comment">#         [ 0,  0],</span></span><br><span class="line"><span class="comment">#         [99,  1]])</span></span><br><span class="line">print(y_view)</span><br><span class="line"><span class="comment"># tensor([[ 1,  1],</span></span><br><span class="line"><span class="comment">#         [ 0,  0],</span></span><br><span class="line"><span class="comment">#         [99,  1]])</span></span><br><span class="line">print(y_transpose)</span><br><span class="line"><span class="comment"># tensor([[ 1,  0],</span></span><br><span class="line"><span class="comment">#         [ 1, 99],</span></span><br><span class="line"><span class="comment">#         [ 0,  1]])</span></span><br><span class="line">print(y_permute)</span><br><span class="line"><span class="comment"># tensor([[ 1,  0],</span></span><br><span class="line"><span class="comment">#         [ 1, 99],</span></span><br><span class="line"><span class="comment">#         [ 0,  1]])</span></span><br></pre></td></tr></table></figure><h2 id="reshape和view的区别"><a href="#reshape和view的区别" class="headerlink" title="reshape和view的区别"></a>reshape和view的区别</h2><p><strong>实验一</strong>：</p><ul><li><p>view使用前要求tensor是连续contiguous的，</p><p>若tensor是不连续的，则view之前必须调用<code>contiguous()</code>方法使之变成连续的tensor</p></li><li><p>permute之后使用contiguous()：</p><p>返回的tensor是x的copy，x变化时，不会随着改变</p><p>返回的tensor是连续的，可以接着使用view变换</p></li><li><p>若<code>y_permute = x.permute(1, 0).contiguous()</code>改为<code>y_permute = x.permute(1, 0)</code>则结论和上述相反，x变化，y_permute跟着变化，view不可以接着使用</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line">x = torch.randint(<span class="number">0</span>, <span class="number">2</span>, (<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">y_permute = x.permute(<span class="number">1</span>, <span class="number">0</span>).contiguous()</span><br><span class="line">print(y_permute.is_contiguous())  <span class="comment"># True</span></span><br><span class="line"></span><br><span class="line">x[<span class="number">1</span>, <span class="number">1</span>] = <span class="number">99</span></span><br><span class="line">print(x)</span><br><span class="line"><span class="comment"># tensor([[ 1,  1,  0],</span></span><br><span class="line"><span class="comment">#         [ 0, 99,  1]])</span></span><br><span class="line">print(y_permute)</span><br><span class="line"><span class="comment"># tensor([[1, 0],</span></span><br><span class="line"><span class="comment">#         [1, 1],</span></span><br><span class="line"><span class="comment">#         [0, 1]])</span></span><br><span class="line"></span><br><span class="line">y_view = y_permute.view(<span class="number">-1</span>)</span><br><span class="line">print(y_view)</span><br><span class="line"><span class="comment"># tensor([1, 0, 1, 1, 0, 1])</span></span><br></pre></td></tr></table></figure><p><strong>实验二：</strong></p><ul><li>tensor是不连续的时候，reshape依然会返回新的copy，view报错</li><li>tensor是连续的时候，view和reshape都不返回新的copy，跟随改变</li><li>view和reshape最终返回的一定是连续的tensor</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line">x = torch.randint(<span class="number">0</span>, <span class="number">2</span>, (<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">y_permute = x.permute(<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">print(y_permute.is_contiguous())  <span class="comment"># False</span></span><br><span class="line"></span><br><span class="line">y_reshape = y_permute.reshape(<span class="number">1</span>, <span class="number">6</span>)</span><br><span class="line">print(y_reshape.is_contiguous())  <span class="comment"># True</span></span><br><span class="line"></span><br><span class="line">x[<span class="number">1</span>, <span class="number">1</span>] = <span class="number">99</span></span><br><span class="line">print(x)</span><br><span class="line"><span class="comment"># tensor([[ 1,  1,  0],</span></span><br><span class="line"><span class="comment">#         [ 0, 99,  1]])</span></span><br><span class="line">print(y_permute)</span><br><span class="line"><span class="comment"># tensor([[ 1,  0],</span></span><br><span class="line"><span class="comment">#         [ 1, 99],</span></span><br><span class="line"><span class="comment">#         [ 0,  1]])</span></span><br><span class="line">print(y_reshape)</span><br><span class="line"><span class="comment"># tensor([[1, 0, 1, 1, 0, 1]])</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>torch.gather用法</title>
      <link href="/2019/07/22/torch.gather/"/>
      <url>/2019/07/22/torch.gather/</url>
      
        <content type="html"><![CDATA[<h2 id="For-a-1-D-tensor"><a href="#For-a-1-D-tensor" class="headerlink" title="For a 1-D tensor"></a>For a 1-D tensor</h2><p>计算公式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output[i] = input[index[i]] <span class="comment"># dim=0</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">input = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])  <span class="comment"># shape = [3]</span></span><br><span class="line">index = torch.tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>])  <span class="comment"># shape = [n]</span></span><br><span class="line">output = input.gather(dim=<span class="number">0</span>, index=index)  <span class="comment"># tensor([1, 2, 2, 2, 2, 2, 3])</span></span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="For-a-2-D-tensor"><a href="#For-a-2-D-tensor" class="headerlink" title="For a 2-D tensor"></a>For a 2-D tensor</h2><p>计算公式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">output[i][j] = input[index[i][j]][j]  <span class="keyword">if</span> dim=<span class="number">0</span></span><br><span class="line">output[i][j] = input[i][index[i][j]]  <span class="keyword">if</span> dim=<span class="number">1</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">input = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                      [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])  <span class="comment"># shape = [2,3]</span></span><br><span class="line">index_dim_0 = torch.tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]])  <span class="comment"># shape = [1,3]</span></span><br><span class="line">output = torch.gather(input, dim=<span class="number">0</span>, index=index_dim_0)  <span class="comment"># tensor([[1, 5, 3]])</span></span><br><span class="line"></span><br><span class="line">index_dim_0 = torch.tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],  <span class="comment"># shape = [2,3]</span></span><br><span class="line">                            [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line"><span class="comment"># [[1, 5, 3],</span></span><br><span class="line"><span class="comment">#  [4, 2, 6]]</span></span><br><span class="line">output = torch.gather(input, dim=<span class="number">0</span>, index=index_dim_0)</span><br><span class="line"></span><br><span class="line">index_dim_1 = torch.tensor([[<span class="number">0</span>],  <span class="comment"># shape = [2,1]</span></span><br><span class="line">                            [<span class="number">2</span>]])</span><br><span class="line"><span class="comment"># tensor([[1],</span></span><br><span class="line"><span class="comment">#         [6]])</span></span><br><span class="line">output = torch.gather(input, dim=<span class="number">1</span>, index=index_dim_1)</span><br><span class="line"></span><br><span class="line">index_dim_1 = torch.tensor([[<span class="number">0</span>, <span class="number">2</span>],  <span class="comment"># shape = [2,2]</span></span><br><span class="line">                            [<span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line"><span class="comment"># tensor([[1, 3],</span></span><br><span class="line"><span class="comment">#         [5, 4]])</span></span><br><span class="line">output = torch.gather(input, dim=<span class="number">1</span>, index=index_dim_1)</span><br><span class="line"></span><br><span class="line">index_dim_1 = torch.tensor([[<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>],  <span class="comment"># shape = [2,3]</span></span><br><span class="line">                            [<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>]])</span><br><span class="line"><span class="comment"># tensor([[3, 2, 1],</span></span><br><span class="line"><span class="comment">#         [5, 4, 6]])</span></span><br><span class="line">output = torch.gather(input, dim=<span class="number">1</span>, index=index_dim_1)</span><br></pre></td></tr></table></figure><h2 id="For-a-3-D-tensor"><a href="#For-a-3-D-tensor" class="headerlink" title="For a 3-D tensor"></a>For a 3-D tensor</h2><p>计算公式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">out[i][j][k] = input[index[i][j][k]][j][k]  <span class="comment"># if dim == 0</span></span><br><span class="line">out[i][j][k] = input[i][index[i][j][k]][k]  <span class="comment"># if dim == 1</span></span><br><span class="line">out[i][j][k] = input[i][j][index[i][j][k]]  <span class="comment"># if dim == 2</span></span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>output.shape == index.shape</li><li>input.shape和index.shape，在dim != d的维度上要保持一致，dim=d的维度任意</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>分类Loss总结</title>
      <link href="/2019/07/22/%E5%88%86%E7%B1%BBLoss%E6%80%BB%E7%BB%93/"/>
      <url>/2019/07/22/%E5%88%86%E7%B1%BBLoss%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h2 id="log-softmax"><a href="#log-softmax" class="headerlink" title="log_softmax"></a>log_softmax</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">x = torch.FloatTensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                       [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])  <span class="comment"># batch=2,C=3,shape=[2,3]</span></span><br><span class="line">y_ours = torch.exp(x)</span><br><span class="line">y_ours = torch.log(y_ours / y_ours.sum(<span class="number">-1</span>).reshape(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">y_log_soft_max = F.log_softmax(x, dim=<span class="number">1</span>)</span><br><span class="line">print(y_ours)</span><br><span class="line">print(y_log_soft_max)</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="BCELoss、BCEWithLogitsLoss"><a href="#BCELoss、BCEWithLogitsLoss" class="headerlink" title="BCELoss、BCEWithLogitsLoss"></a>BCELoss、BCEWithLogitsLoss</h2><p><strong>BCELoss 公式：</strong></p><p>用于二分类问题</p><p><img src="https://i.loli.net/2019/07/22/5d356b724c3ee48819.png" alt></p><p><strong>BCEWithLogitsLoss：</strong></p><p>等价于 sigmoid +  BCELoss，使得数值计算更加稳定</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">input = torch.randn(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">target = torch.empty(<span class="number">3</span>, <span class="number">3</span>).random_(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">loss_bce = nn.BCELoss()</span><br><span class="line">output1 = loss_bce(torch.sigmoid(input), target)</span><br><span class="line"></span><br><span class="line">loss_bcel = nn.BCEWithLogitsLoss()</span><br><span class="line">output2 = loss_bcel(input, target)</span><br><span class="line">print(output1, output2)</span><br></pre></td></tr></table></figure><h2 id="NLLLoss、-CrossEntropyLoss"><a href="#NLLLoss、-CrossEntropyLoss" class="headerlink" title="NLLLoss、 CrossEntropyLoss"></a>NLLLoss、 CrossEntropyLoss</h2><p><strong>NLLLoss：</strong></p><p>三个重要参数：weight，reduction，ignore_index</p><p>通过设置weight参数，可以灵活调控不同类别的loss权重，解决不同类别的数量不均衡问题：</p><ul><li>图像分类：不同的图像类别</li><li>语义分割：不同的像素类别，如：背景像素、前景像素；医学图像中肝脏像素和血管像素等</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">x = torch.FloatTensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                       [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])  <span class="comment"># batch=2,C=3,shape=[2,3]</span></span><br><span class="line">y_log_soft_max = F.log_softmax(x, dim=<span class="number">1</span>)</span><br><span class="line">label = torch.LongTensor([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># reduction none</span></span><br><span class="line">loss_nll = nn.NLLLoss(reduction=<span class="string">'none'</span>)</span><br><span class="line">loss = loss_nll(y_log_soft_max, label)</span><br><span class="line">print(loss)  <span class="comment"># [2.4076, 1.4076]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># reduction mean</span></span><br><span class="line">loss_nll = nn.NLLLoss(reduction=<span class="string">'mean'</span>)</span><br><span class="line">loss = loss_nll(y_log_soft_max, label)</span><br><span class="line">print(loss)  <span class="comment"># 1.9076</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># reduction sum</span></span><br><span class="line">loss_nll = nn.NLLLoss(reduction=<span class="string">'sum'</span>)</span><br><span class="line">loss = loss_nll(y_log_soft_max, label)</span><br><span class="line">print(loss)  <span class="comment"># 3.8152</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ignore_index</span></span><br><span class="line">loss_nll = nn.NLLLoss(reduction=<span class="string">'none'</span>, ignore_index=<span class="number">0</span>)</span><br><span class="line">loss = loss_nll(y_log_soft_max, label)</span><br><span class="line">print(loss)  <span class="comment"># [0.0000, 1.4076]</span></span><br><span class="line"></span><br><span class="line">loss_nll = nn.NLLLoss(reduction=<span class="string">'none'</span>, ignore_index=<span class="number">1</span>)</span><br><span class="line">loss = loss_nll(y_log_soft_max, label)</span><br><span class="line">print(loss)  <span class="comment"># [2.4076, 0.0000]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># weight</span></span><br><span class="line">weight = torch.FloatTensor([<span class="number">2.0</span>, <span class="number">1.0</span>, <span class="number">3.0</span>])  <span class="comment"># class weight</span></span><br><span class="line"></span><br><span class="line">loss_nll = nn.NLLLoss(weight=weight, reduction=<span class="string">'none'</span>)</span><br><span class="line">loss = loss_nll(y_log_soft_max, label)</span><br><span class="line">print(loss)  <span class="comment"># [2.4076*2.0, 1.4076*1.0]</span></span><br><span class="line"></span><br><span class="line">loss_nll = nn.NLLLoss(weight=weight, reduction=<span class="string">'mean'</span>)</span><br><span class="line">loss = loss_nll(y_log_soft_max, label)</span><br><span class="line">print(loss)  <span class="comment"># (2.4076*2.0 + 1.4076*1.0) / (2.0 + 1.0) = 2.0743</span></span><br><span class="line"></span><br><span class="line">loss_nll = nn.NLLLoss(weight=weight, reduction=<span class="string">'sum'</span>)</span><br><span class="line">loss = loss_nll(y_log_soft_max, label)</span><br><span class="line">print(loss)  <span class="comment"># 6.2228</span></span><br></pre></td></tr></table></figure><p><strong>CrossEntropyLoss：</strong></p><p>等价于 log_softmax + NLLLoss</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = torch.FloatTensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                       [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])  <span class="comment"># batch=2,C=3,shape=[2,3]</span></span><br><span class="line">label = torch.LongTensor([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">loss_cross_entropy = nn.CrossEntropyLoss()</span><br><span class="line">print(loss_cross_entropy(x, label))  <span class="comment"># 1.9076</span></span><br><span class="line"></span><br><span class="line">loss_nll = nn.NLLLoss()</span><br><span class="line">print(loss_nll(F.log_softmax(x, dim=<span class="number">1</span>), label))  <span class="comment"># 1.9076</span></span><br></pre></td></tr></table></figure><h2 id="Segmentation-2d-3d"><a href="#Segmentation-2d-3d" class="headerlink" title="Segmentation 2d/3d"></a>Segmentation 2d/3d</h2><p>分类loss在分割问题上的应用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2D input is of size N x C x height x width</span></span><br><span class="line">N, C = <span class="number">4</span>, <span class="number">10</span></span><br><span class="line">x = torch.randn(N, <span class="number">1</span>, <span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line">target = torch.empty(N, <span class="number">64</span>, <span class="number">64</span>, dtype=torch.long).random_(<span class="number">0</span>, C)</span><br><span class="line"></span><br><span class="line">model = nn.Conv2d(<span class="number">1</span>, C, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="number">1</span>)</span><br><span class="line">output = model(x)  <span class="comment"># [4, 10, 64, 64]</span></span><br><span class="line"></span><br><span class="line">loss_nll = nn.NLLLoss()</span><br><span class="line">loss = loss_nll(F.log_softmax(output, dim=<span class="number">1</span>), target)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3D input is of size N x C x height x width × Z</span></span><br><span class="line">N, C = <span class="number">4</span>, <span class="number">10</span></span><br><span class="line">x = torch.randn(N, <span class="number">1</span>, <span class="number">64</span>, <span class="number">64</span>, <span class="number">8</span>)</span><br><span class="line">target = torch.empty(N, <span class="number">64</span>, <span class="number">64</span>, <span class="number">8</span>, dtype=torch.long).random_(<span class="number">0</span>, C)</span><br><span class="line"></span><br><span class="line">model = nn.Conv3d(<span class="number">1</span>, C, (<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>), padding=<span class="number">1</span>)</span><br><span class="line">output = model(x)  <span class="comment"># [4, 10, 64, 64, 8]</span></span><br><span class="line"></span><br><span class="line">loss_nll = nn.NLLLoss()</span><br><span class="line">loss = loss_nll(F.log_softmax(output, dim=<span class="number">1</span>), target)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
