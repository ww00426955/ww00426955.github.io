<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[BatchNorm总结]]></title>
    <url>%2F2019%2F07%2F29%2FBN%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[参考 https://zhuanlan.zhihu.com/p/26138673 动机 一般来说，如果模型的输入特征不相关且满足标准正态分布时，模型的表现一般较好。在训练神经网络模型时，通过事先将特征去相关并使得它们满足一个比较好的分布，这样，模型的第一层网络一般都会有一个比较好的输入特征。但是随着模型的层数加深，网络的非线性变换使得每一层的结果变得相关了，且不再满足分布。更糟糕的是，可能这些隐藏层的特征分布已经发生了偏移。 论文的作者认为上面的问题，导致神经网络的训练变得困难。基于此，提出了在层和层之间加入Batch Normalization层来解决这一问题。 训练时，BN层利用隐藏层输出结果的均值与方差来标准化每一层特征的分布，并且维护所有mini-batch数据的均值与方差，最后利用样本的均值与方差的无偏估计量用于测试时使用。 鉴于在某些情况下非标准化分布的层特征可能是最优，标准化每一层的输出特征反而会使得网络的表达能力变差，因此添加两个可学习的缩放参数和偏移参数来允许模型自适应地调整层特征分布。 作用 模型训练收敛的速度更快 模型隐藏输出特征的分布更稳定，更利于模型的学习，达到更高的准确率 公式推导 前向传播 反向传播 两种模式训练模式： 滑动平均 滑动平均(exponential moving average)，或者叫做指数加权平均(exponentially weighted moving average)，可以用来估计变量的局部均值，使得变量的更新与一段时间内的历史取值有关 占内存少，不需要保存过去10个或者100个历史值，就能够估计均值。（当然，滑动平均不如将历史值全保存下来计算均值准确，但后者占用更多内存和计算成本更高） 计算当前Batch的均值和方差，然后使用滑动平均策略来更新：running_mean和running_var 有2*feature_num个可以学习的参数γ和β 测试模式： 使用训练得到的running_mean和running_var，作为样本的统计量，进行前向计算 代码验证： 若输入维度为：B×C×H×W，则BN计算的是B×H×W个值的均值和方差 BN前的卷积层不需要bias参数 123456789101112131415161718192021222324252627import torchimport torch.nn as nnb, c, h, w = 2, 2, 2, 2conv_out = torch.empty((b, c, h, w)).random_(0, 4) # B*C*H*Wprint('batch---')batch1 = conv_out[:, 0, :, :] # B*H*Wbatch2 = conv_out[:, 1, :, :] # B*H*Wprint('mean: [&#123;:.4f&#125; &#123;:.4f&#125;]'.format(batch1.mean().item(), batch2.mean().item()))print('var : [&#123;:.4f&#125; &#123;:.4f&#125;]'.format(batch1.var().item(), batch2.var().item()))bn = nn.BatchNorm2d(c, momentum=0.1)print('before BN---')# for p in bn.parameters():# print(p)print('mean: &#123;&#125;'.format(bn.running_mean.numpy()))print('var : &#123;&#125;'.format(bn.running_var.numpy()))bn_out = bn(conv_out)print('after BN once---')print('mean: &#123;&#125;'.format(bn.running_mean.numpy()))print('var : &#123;&#125;'.format(bn.running_var.numpy()))bn_out = bn(conv_out)print('after BN twice---')print('mean: &#123;&#125;'.format(bn.running_mean.numpy()))print('var : &#123;&#125;'.format(bn.running_var.numpy())) 实现前向和BP，根据公式 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293import numpy as npdef batchnorm_forward(x, gamma, beta, bn_param): """ Forward pass for batch normalization. Input: - x: Data of shape (N, D) - gamma: Scale parameter of shape (D,) - beta: Shift paremeter of shape (D,) - bn_param: Dictionary with the following keys: - mode: 'train' or 'test'; required - eps: Constant for numeric stability - momentum: Constant for running mean / variance. - running_mean: Array of shape (D,) giving running mean of features - running_var Array of shape (D,) giving running variance of features Returns a tuple of: - out: of shape (N, D) - cache: A tuple of values needed in the backward pass """ mode = bn_param['mode'] eps = bn_param.get('eps', 1e-5) momentum = bn_param.get('momentum', 0.9) N, D = x.shape running_mean = bn_param.get('running_mean', np.zeros(D, dtype=x.dtype)) running_var = bn_param.get('running_var', np.zeros(D, dtype=x.dtype)) out, cache = None, None if mode == 'train': sample_mean = np.mean(x, axis=0) sample_var = np.var(x, axis=0) out_ = (x - sample_mean) / np.sqrt(sample_var + eps) running_mean = momentum * running_mean + (1 - momentum) * sample_mean running_var = momentum * running_var + (1 - momentum) * sample_var out = gamma * out_ + beta cache = (out_, x, sample_var, sample_mean, eps, gamma, beta) elif mode == 'test': scale = gamma / np.sqrt(running_var + eps) out = (x - running_mean) * scale + beta # out = x * scale + (beta - running_mean * scale) else: raise ValueError('Invalid forward batchnorm mode "%s"' % mode) # Store the updated running means back into bn_param bn_param['running_mean'] = running_mean bn_param['running_var'] = running_var return out, cachedef batchnorm_backward(dout, cache): """ Backward pass for batch normalization. Inputs: - dout: Upstream derivatives, of shape (N, D) - cache: Variable of intermediates from batchnorm_forward. Returns a tuple of: - dx: Gradient with respect to inputs x, of shape (N, D) - dgamma: Gradient with respect to scale parameter gamma, of shape (D,) - dbeta: Gradient with respect to shift parameter beta, of shape (D,) """ dx, dgamma, dbeta = None, None, None out_, x, sample_var, sample_mean, eps, gamma, beta = cache N = x.shape[0] dout_ = gamma * dout dvar = np.sum(dout_ * (x - sample_mean) * -0.5 * (sample_var + eps) ** -1.5, axis=0) dx_ = 1 / np.sqrt(sample_var + eps) dvar_ = 2 * (x - sample_mean) / N # intermediate for convenient calculation di = dout_ * dx_ + dvar * dvar_ dmean = -1 * np.sum(di, axis=0) dmean_ = np.ones_like(x) / N dx = di + dmean * dmean_ dgamma = np.sum(dout * out_, axis=0) dbeta = np.sum(dout, axis=0) return dx, dgamma, dbeta BN变种 不同的Norm本质上的区别是，用于计算均值和方差的特征点的集合不同 除了BN，其他的Norm都是Batch无关的 BN适用于判别模型，比如图像分类，由于在Batch维度进行归一化，可以认为是使用的是数据的整体分布，进而保证不同数据的分布一致性。而判别模型的结果正是取决于数据的整体分布。 IN适用于生成模型，如风格迁移。图片生成的结果主要依赖于单个图像实例，所以Batch的归一化操作就不适合了。在风格迁移中IN可以加速收敛，保证每个图像实例之间的独立性。]]></content>
  </entry>
  <entry>
    <title><![CDATA[分类Loss总结]]></title>
    <url>%2F2019%2F07%2F23%2F%E5%88%86%E7%B1%BBLoss%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[log_softmax1234567891011import torchimport torch.nn as nnimport torch.nn.functional as Fx = torch.FloatTensor([[1, 2, 3], [4, 5, 6]]) # batch=2,C=3,shape=[2,3]y_ours = torch.exp(x)y_ours = torch.log(y_ours / y_ours.sum(-1).reshape(2, 1))y_log_soft_max = F.log_softmax(x, dim=1)print(y_ours)print(y_log_soft_max) BCELoss、BCEWithLogitsLossBCELoss 公式： 用于二分类问题 BCEWithLogitsLoss： 等价于 sigmoid + BCELoss，使得数值计算更加稳定 123456789101112import torchimport torch.nn as nninput = torch.randn(3, 3)target = torch.empty(3, 3).random_(2)loss_bce = nn.BCELoss()output1 = loss_bce(torch.sigmoid(input), target)loss_bcel = nn.BCEWithLogitsLoss()output2 = loss_bcel(input, target)print(output1, output2) NLLLoss、 CrossEntropyLossNLLLoss： 三个重要参数：weight，reduction，ignore_index 通过设置weight参数，可以灵活调控不同类别的loss权重，解决不同类别的数量不均衡问题： 图像分类：不同的图像类别 语义分割：不同的像素类别，如：背景像素、前景像素；医学图像中肝脏像素和血管像素等 12345678910111213141516171819202122232425262728293031323334353637383940414243x = torch.FloatTensor([[1, 2, 3], [4, 5, 6]]) # batch=2,C=3,shape=[2,3]y_log_soft_max = F.log_softmax(x, dim=1)label = torch.LongTensor([0, 1])# reduction noneloss_nll = nn.NLLLoss(reduction='none')loss = loss_nll(y_log_soft_max, label)print(loss) # [2.4076, 1.4076]# reduction meanloss_nll = nn.NLLLoss(reduction='mean')loss = loss_nll(y_log_soft_max, label)print(loss) # 1.9076# reduction sumloss_nll = nn.NLLLoss(reduction='sum')loss = loss_nll(y_log_soft_max, label)print(loss) # 3.8152# ignore_indexloss_nll = nn.NLLLoss(reduction='none', ignore_index=0)loss = loss_nll(y_log_soft_max, label)print(loss) # [0.0000, 1.4076]loss_nll = nn.NLLLoss(reduction='none', ignore_index=1)loss = loss_nll(y_log_soft_max, label)print(loss) # [2.4076, 0.0000]# weightweight = torch.FloatTensor([2.0, 1.0, 3.0]) # class weightloss_nll = nn.NLLLoss(weight=weight, reduction='none')loss = loss_nll(y_log_soft_max, label)print(loss) # [2.4076*2.0, 1.4076*1.0]loss_nll = nn.NLLLoss(weight=weight, reduction='mean')loss = loss_nll(y_log_soft_max, label)print(loss) # (2.4076*2.0 + 1.4076*1.0) / (2.0 + 1.0) = 2.0743loss_nll = nn.NLLLoss(weight=weight, reduction='sum')loss = loss_nll(y_log_soft_max, label)print(loss) # 6.2228 CrossEntropyLoss： 等价于 log_softmax + NLLLoss 123456789x = torch.FloatTensor([[1, 2, 3], [4, 5, 6]]) # batch=2,C=3,shape=[2,3]label = torch.LongTensor([0, 1])loss_cross_entropy = nn.CrossEntropyLoss()print(loss_cross_entropy(x, label)) # 1.9076loss_nll = nn.NLLLoss()print(loss_nll(F.log_softmax(x, dim=1), label)) # 1.9076 Segmentation 2d/3d分类loss在分割问题上的应用： 12345678910111213141516171819202122232425import torchimport torch.nn as nnimport torch.nn.functional as F# 2D input is of size N x C x height x widthN, C = 4, 10x = torch.randn(N, 1, 64, 64)target = torch.empty(N, 64, 64, dtype=torch.long).random_(0, C)model = nn.Conv2d(1, C, (3, 3), padding=1)output = model(x) # [4, 10, 64, 64]loss_nll = nn.NLLLoss()loss = loss_nll(F.log_softmax(output, dim=1), target)# 3D input is of size N x C x height x width × ZN, C = 4, 10x = torch.randn(N, 1, 64, 64, 8)target = torch.empty(N, 64, 64, 8, dtype=torch.long).random_(0, C)model = nn.Conv3d(1, C, (3, 3, 3), padding=1)output = model(x) # [4, 10, 64, 64, 8]loss_nll = nn.NLLLoss()loss = loss_nll(F.log_softmax(output, dim=1), target)]]></content>
  </entry>
  <entry>
    <title><![CDATA[反向传播推导]]></title>
    <url>%2F2019%2F07%2F23%2FsoftmaxBP%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[Softmax + 交叉熵参考： https://zhuanlan.zhihu.com/p/21485970 https://upload-images.jianshu.io/upload_images/2301760-1c7b8c12bbe6a1bc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 手推1： 手推2： 代码验证： 12345678910111213141516171819202122import torchimport torch.nn as nnimport torch.nn.functional as F# soft_max + 交叉熵的bp推导# batch = 1 class_num = 3z = torch.tensor([[4.0, 5.0, 6.0]], requires_grad=True) # shape [1, 3]y_output = F.softmax(z, dim=1)print(y_output) # [[0.0900, 0.2447, 0.6652]]y_output_log = torch.log(y_output)print(y_output_log) # [[-2.4076, -1.4076, -0.4076]]# 计算lossy_label = torch.tensor([2])loss_nll = nn.NLLLoss(reduction='none')loss = loss_nll(y_output_log, y_label)print(loss.item()) # 0.4076 == -y_output_log[2]# 反向传播loss.backward()print(z.grad) # [[ 0.0900, 0.2447, -0.3348]] == [[0.0900, 0.2447, 0.6652]] - [[0, 0, 1]]]]></content>
  </entry>
  <entry>
    <title><![CDATA[pytorch之维度变化]]></title>
    <url>%2F2019%2F07%2F22%2Fpytorch%E4%B9%8B%E7%BB%B4%E5%BA%A6%E5%8F%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[reshape、view和transpose、permute的区别 reshape和view只是变换形状,而transpose和permute是变换维度，变换结果不一样！ reshape和view变换后的得到的tensor都是连续的，而transpose和permute是不连续的 若x是连续的tensor，则x的改变都会影响以上的变换结果，因为都不是x的副本 1234567891011121314151617181920212223242526272829303132333435import torchtorch.manual_seed(1)x = torch.randint(0, 2, (2, 3))y_reshape = x.reshape(3, 2)y_view = x.view(3, 2)print(y_reshape.is_contiguous()) # Trueprint(y_view.is_contiguous()) # Truey_transpose = x.transpose(0, 1)y_permute = x.permute(1, 0)print(y_transpose.is_contiguous()) # Falseprint(y_permute.is_contiguous()) # Falsex[1, 1] = 99print(x)# tensor([[ 1, 1, 0],# [ 0, 99, 1]])print(y_reshape)# tensor([[ 1, 1],# [ 0, 0],# [99, 1]])print(y_view)# tensor([[ 1, 1],# [ 0, 0],# [99, 1]])print(y_transpose)# tensor([[ 1, 0],# [ 1, 99],# [ 0, 1]])print(y_permute)# tensor([[ 1, 0],# [ 1, 99],# [ 0, 1]]) reshape和view的区别实验一： view使用前要求tensor是连续contiguous的， 若tensor是不连续的，则view之前必须调用contiguous()方法使之变成连续的tensor permute之后使用contiguous()： 返回的tensor是x的copy，x变化时，不会随着改变 返回的tensor是连续的，可以接着使用view变换 若y_permute = x.permute(1, 0).contiguous()改为y_permute = x.permute(1, 0)则结论和上述相反，x变化，y_permute跟着变化，view不可以接着使用 1234567891011121314151617181920import torchtorch.manual_seed(1)x = torch.randint(0, 2, (2, 3))y_permute = x.permute(1, 0).contiguous()print(y_permute.is_contiguous()) # Truex[1, 1] = 99print(x)# tensor([[ 1, 1, 0],# [ 0, 99, 1]])print(y_permute)# tensor([[1, 0],# [1, 1],# [0, 1]])y_view = y_permute.view(-1)print(y_view)# tensor([1, 0, 1, 1, 0, 1]) 实验二： tensor是不连续的时候，reshape依然会返回新的copy，view报错 tensor是连续的时候，view和reshape都不返回新的copy，跟随改变 view和reshape最终返回的一定是连续的tensor 123456789101112131415161718192021import torchtorch.manual_seed(1)x = torch.randint(0, 2, (2, 3))y_permute = x.permute(1, 0)print(y_permute.is_contiguous()) # Falsey_reshape = y_permute.reshape(1, 6)print(y_reshape.is_contiguous()) # Truex[1, 1] = 99print(x)# tensor([[ 1, 1, 0],# [ 0, 99, 1]])print(y_permute)# tensor([[ 1, 0],# [ 1, 99],# [ 0, 1]])print(y_reshape)# tensor([[1, 0, 1, 1, 0, 1]])]]></content>
  </entry>
  <entry>
    <title><![CDATA[torch.gather用法]]></title>
    <url>%2F2019%2F07%2F22%2Ftorch.gather%2F</url>
    <content type="text"><![CDATA[For a 1-D tensor计算公式： 1output[i] = input[index[i]] # dim=0 1234import torchinput = torch.tensor([1, 2, 3]) # shape = [3]index = torch.tensor([0, 1, 1, 1, 1, 1, 2]) # shape = [n]output = input.gather(dim=0, index=index) # tensor([1, 2, 2, 2, 2, 2, 3]) For a 2-D tensor计算公式： 12output[i][j] = input[index[i][j]][j] if dim=0output[i][j] = input[i][index[i][j]] if dim=1 1234567891011121314151617181920212223242526272829import torchinput = torch.tensor([[1, 2, 3], [4, 5, 6]]) # shape = [2,3]index_dim_0 = torch.tensor([[0, 1, 0]]) # shape = [1,3]output = torch.gather(input, dim=0, index=index_dim_0) # tensor([[1, 5, 3]])index_dim_0 = torch.tensor([[0, 1, 0], # shape = [2,3] [1, 0, 1]])# [[1, 5, 3],# [4, 2, 6]]output = torch.gather(input, dim=0, index=index_dim_0)index_dim_1 = torch.tensor([[0], # shape = [2,1] [2]])# tensor([[1],# [6]])output = torch.gather(input, dim=1, index=index_dim_1)index_dim_1 = torch.tensor([[0, 2], # shape = [2,2] [1, 0]])# tensor([[1, 3],# [5, 4]])output = torch.gather(input, dim=1, index=index_dim_1)index_dim_1 = torch.tensor([[2, 1, 0], # shape = [2,3] [1, 0, 2]])# tensor([[3, 2, 1],# [5, 4, 6]])output = torch.gather(input, dim=1, index=index_dim_1) For a 3-D tensor计算公式： 123out[i][j][k] = input[index[i][j][k]][j][k] # if dim == 0out[i][j][k] = input[i][index[i][j][k]][k] # if dim == 1out[i][j][k] = input[i][j][index[i][j][k]] # if dim == 2 总结 output.shape == index.shape input.shape和index.shape，在dim != d的维度上要保持一致，dim=d的维度任意]]></content>
  </entry>
</search>
