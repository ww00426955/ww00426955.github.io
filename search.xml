<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title></title>
    <url>%2F2019%2F07%2F21%2F%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[ssh deepliver4@192.168.1.111 ssh deepliver-0@192.168.1.104 ssh deepliver8@192.168.1.115 自己的电脑 TeamViewer：1124546805 deepliver]]></content>
  </entry>
  <entry>
    <title><![CDATA[NLL和CrossEntropyLoss]]></title>
    <url>%2F2019%2F07%2F21%2FNLL%E5%92%8CCrossEntropyLoss%2F</url>
    <content type="text"><![CDATA[1. log_softmax 1234567891011import torchimport torch.nn as nnimport torch.nn.functional as Fx = torch.FloatTensor([[1, 2, 3], [4, 5, 6]]) # batch=2,C=3,shape=[2,3]y_ours = torch.exp(x)y_ours = torch.log(y_ours / y_ours.sum(-1).reshape(2, 1))y_log_soft_max = F.log_softmax(x, dim=1)print(y_ours)print(y_log_soft_max) 2. NLLLoss 三个重要参数：weight，reduction，ignore_index 12345678910111213141516171819202122232425262728293031323334353637383940414243x = torch.FloatTensor([[1, 2, 3], [4, 5, 6]]) # batch=2,C=3,shape=[2,3]y_log_soft_max = F.log_softmax(x, dim=1)label = torch.LongTensor([0, 1])# reduction noneloss_nll = nn.NLLLoss(reduction='none')loss = loss_nll(y_log_soft_max, label)print(loss) # [2.4076, 1.4076]# reduction meanloss_nll = nn.NLLLoss(reduction='mean')loss = loss_nll(y_log_soft_max, label)print(loss) # 1.9076# reduction sumloss_nll = nn.NLLLoss(reduction='sum')loss = loss_nll(y_log_soft_max, label)print(loss) # 3.8152# ignore_indexloss_nll = nn.NLLLoss(reduction='none', ignore_index=0)loss = loss_nll(y_log_soft_max, label)print(loss) # [0.0000, 1.4076]loss_nll = nn.NLLLoss(reduction='none', ignore_index=1)loss = loss_nll(y_log_soft_max, label)print(loss) # [2.4076, 0.0000]# weightweight = torch.FloatTensor([2.0, 1.0, 3.0]) # class weightloss_nll = nn.NLLLoss(weight=weight, reduction='none')loss = loss_nll(y_log_soft_max, label)print(loss) # [2.4076*2.0, 1.4076*1.0]loss_nll = nn.NLLLoss(weight=weight, reduction='mean')loss = loss_nll(y_log_soft_max, label)print(loss) # (2.4076*2.0 + 1.4076*1.0) / (2.0 + 1.0) = 2.0743loss_nll = nn.NLLLoss(weight=weight, reduction='sum')loss = loss_nll(y_log_soft_max, label)print(loss) # 6.2228 3. NLLLoss and CrossEntropyLoss 123456789x = torch.FloatTensor([[1, 2, 3], [4, 5, 6]]) # batch=2,C=3,shape=[2,3]label = torch.LongTensor([0, 1])loss_cross_entropy = nn.CrossEntropyLoss()print(loss_cross_entropy(x, label)) # 1.9076loss_nll = nn.NLLLoss()print(loss_nll(F.log_softmax(x, dim=1), label)) # 1.9076 4. Segmentation 2d/3d 12345678910111213141516171819202122232425import torchimport torch.nn as nnimport torch.nn.functional as F# 2D input is of size N x C x height x widthN, C = 4, 10x = torch.randn(N, 1, 64, 64)target = torch.empty(N, 64, 64, dtype=torch.long).random_(0, C)model = nn.Conv2d(1, C, (3, 3), padding=1)output = model(x) # [4, 10, 64, 64]loss_nll = nn.NLLLoss()loss = loss_nll(F.log_softmax(output, dim=1), target)# 3D input is of size N x C x height x width × ZN, C = 4, 10x = torch.randn(N, 1, 64, 64, 8)target = torch.empty(N, 64, 64, 8, dtype=torch.long).random_(0, C)model = nn.Conv3d(1, C, (3, 3, 3), padding=1)output = model(x) # [4, 10, 64, 64, 8]loss_nll = nn.NLLLoss()loss = loss_nll(F.log_softmax(output, dim=1), target) 总结 CrossEntropyLoss 等价于 log_softmax + NLLLoss 通过设置weight参数，可以灵活调控不同类别的loss权重，解决不同类别的数量不均衡问题： 图像分类：不同的图像类别 语义分割：不同的像素类别，如：背景像素、前景像素；医学图像中肝脏像素和血管像素等]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
</search>
